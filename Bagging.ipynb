{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings # отключим предупреждения Anaconda\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "%config InlineBackend.figure_format = 'svg' #графики в svg выглядят более четкими\n",
    "from pylab import rcParams #увеличим дефолтный размер графиков\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.graph_objects as go\n",
    "init_notebook_mode(connected=True)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.options.display.float_format = '{:,.1f}'.format # Отображение чисел float с запятыми и 1 знаком после точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bigml_59c28831336c6604c800002a.csv')\n",
    "d = {'no' : False, 'yes' : True}\n",
    "data['international plan'] = data['international plan'].map(d)\n",
    "data['voice mail plan'] = data['voice mail plan'].map(d)\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sns.kdeplot(telecom_data[telecom_data['churn'] == False]['customer service calls'], label = 'Loyal')\n",
    "fig = sns.kdeplot(telecom_data[telecom_data['churn'] == True]['customer service calls'], label = 'Churn')        \n",
    "fig.set(xlabel='Количество звонков', ylabel='Плотность')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples):\n",
    "    # функция для генерации подвыборок с помощью бутстрэпа\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples\n",
    "def stat_intervals(stat, alpha):\n",
    "    # функция для интервальной оценки\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries\n",
    "\n",
    "# сохранение в отдельные numpy массивы данных по лояльным и уже бывшим клиентам\n",
    "loyal_calls = telecom_data[telecom_data['churn'] == False]['customer service calls'].values\n",
    "churn_calls= telecom_data[telecom_data['churn'] == True]['customer service calls'].values\n",
    "\n",
    "# ставим seed для воспроизводимости результатов\n",
    "np.random.seed(0)\n",
    "\n",
    "# генерируем выборки с помощью бутстрэра и сразу считаем по каждой из них среднее\n",
    "loyal_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(loyal_calls, 1000)]\n",
    "churn_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(churn_calls, 1000)]\n",
    "\n",
    "#  выводим интервальную оценку среднего\n",
    "print(\"Service calls from loyal:  mean interval\",  stat_intervals(loyal_mean_scores, 0.05))\n",
    "print(\"Service calls from churn:  mean interval\",  stat_intervals(churn_mean_scores, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 150        \n",
    "n_test = 1000       \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\\\n",
    "        + np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "# One decision tree regressor\n",
    "dtree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "d_predict = dtree.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, d_predict, \"g\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Решающее дерево, MSE = %.2f\" \n",
    "          % np.sum((y_test - d_predict) ** 2))\n",
    "\n",
    "# Bagging decision tree regressor\n",
    "bdt = BaggingRegressor(DecisionTreeRegressor()).fit(X_train, y_train)\n",
    "bdt_predict = bdt.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, bdt_predict, \"y\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Бэггинг решающих деревьев, MSE = %.2f\" % np.sum((y_test - bdt_predict) ** 2));\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=10).fit(X_train, y_train)\n",
    "rf_predict = rf.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, rf_predict, \"r\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Случайный лес, MSE = %.2f\" % np.sum((y_test - rf_predict) ** 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем сначала только колонки с числовым типом данных\n",
    "cols = []\n",
    "for i in df.columns:\n",
    "    if (df[i].dtype == \"float64\") or (df[i].dtype == 'int64'):\n",
    "        cols.append(i)\n",
    "\n",
    "# Разделяем на признаки и объекты\n",
    "X, y = df[cols].copy(), np.asarray(df[\"churn\"],dtype='int8')\n",
    "\n",
    "# Инициализируем страифицированную разбивку нашего датасета для валидации\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Инициализируем наш классификатор с дефолтными параметрами\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "results = cross_val_score(rfc, X, y, cv=skf)\n",
    "\n",
    "# Оцениваем долю верных ответов на тестовом датасете\n",
    "print(\"CV accuracy score: {:.2f}%\".format(results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем валидацию\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "trees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for ntrees in trees_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "\n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} trees\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        trees_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"N_estimators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "max_depth_grid = [3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for max_depth in max_depth_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, oob_score=True, max_depth=max_depth)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "\n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} max_depth\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        max_depth_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_depth_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(max_depth_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем валидацию\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "min_samples_leaf_grid = [1, 3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for min_samples_leaf in min_samples_leaf_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                 oob_score=True, min_samples_leaf=min_samples_leaf)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "\n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} min_samples_leaf\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        min_samples_leaf_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(min_samples_leaf_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(min_samples_leaf_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Min_samples_leaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем инициализацию параметров, по которым хотим сделать полный перебор\n",
    "parameters = {'max_features': [4, 7, 10, 13], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [5,10,15,20]}\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "gcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostel_data = pd.read_csv(\"hostel_factors.csv\")\n",
    "features = {\"f1\":u\"Персонал\",\n",
    "\"f2\":u\"Бронирование хостела \",\n",
    "\"f3\":u\"Заезд в хостел и выезд из хостела\",\n",
    "\"f4\":u\"Состояние комнаты\",\n",
    "\"f5\":u\"Состояние общей кухни\",\n",
    "\"f6\":u\"Состояние общего пространства\",\n",
    "\"f7\":u\"Дополнительные услуги\",\n",
    "\"f8\":u\"Общие условия и удобства\",\n",
    "\"f9\":u\"Цена/качество\",\n",
    "\"f10\":u\"ССЦ\"}\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=1000, max_features=10,\n",
    "                                random_state=0)\n",
    "\n",
    "forest.fit(hostel_data.drop(['hostel', 'rating'], axis=1), \n",
    "           hostel_data['rating'])\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Plot the feature importancies of the forest\n",
    "num_to_plot = 10\n",
    "feature_indices = [ind+1 for ind in indices[:num_to_plot]]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(num_to_plot):\n",
    "    print(\"%d. %s %f \" % (f + 1, \n",
    "            features[\"f\"+str(feature_indices[f])], \n",
    "            importances[indices[f]]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(u\"Важность конструктов\")\n",
    "bars = plt.bar(range(num_to_plot), \n",
    "               importances[indices[:num_to_plot]],\n",
    "       color=([str(i/float(num_to_plot+1)) \n",
    "               for i in range(num_to_plot)]),\n",
    "               align=\"center\")\n",
    "ticks = plt.xticks(range(num_to_plot), \n",
    "                   feature_indices)\n",
    "plt.xlim([-1, num_to_plot])\n",
    "plt.legend(bars, [u''.join(features[\"f\"+str(i)]) \n",
    "                  for i in feature_indices]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
